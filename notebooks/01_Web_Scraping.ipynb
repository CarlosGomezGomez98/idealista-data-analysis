{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Idealista Web Scraper\n",
    "\n",
    "**Author:** Carlos Gómez Gómez\n",
    "\n",
    "This notebook contains the complete code for scraping property data from the Spanish real estate portal, `Idealista`. The process is divided into two main steps:\n",
    "1.  **Fetching Property IDs:** First, we scrape the listing pages for a specific zone to collect the unique ID of each property.\n",
    "2.  **Parsing Property Details:** Then, we visit each individual property page to extract detailed information, such as price, size, number of rooms, and other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Library Imports ---\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import undetected_chromedriver as uc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Scraping Functions\n",
    "\n",
    "Here we define the core functions responsible for the data extraction process.\n",
    "\n",
    "### 2.1 Get Property IDs\n",
    "\n",
    "The `get_id_list` function automates Browse through Idealista's paginated search results for a given geographical zone. It simulates user behavior by navigating page by page, handling cookie banners, and extracting the unique `data-element-id` for each property listing it finds. Finally, it returns a clean list of unique IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_list(target_zone):\n",
    "    \"\"\"\n",
    "    Scrapes Idealista for a given search zone to retrieve a list of all property IDs.\n",
    "    \n",
    "    Args:\n",
    "        target_zone (str): The specific search path for the zone (e.g., 'barcelona/sarria-sant-gervasi').\n",
    "\n",
    "    Returns:\n",
    "        list: A list of unique property IDs.\n",
    "    \"\"\"\n",
    "    page_number = 1\n",
    "    id_list = []\n",
    "    driver = uc.Chrome()\n",
    "    \n",
    "    while True:\n",
    "        if page_number == 1:\n",
    "            url = f'https://www.idealista.com/en/venta-viviendas/{target_zone}/'\n",
    "        else:\n",
    "            url = f'https://www.idealista.com/en/venta-viviendas/{target_zone}/pagina-{page_number}.htm'\n",
    "        \n",
    "        driver.get(url)\n",
    "        print(f\"Scraping page {page_number} for IDs...\")\n",
    "        time.sleep(random.randint(8, 12))\n",
    "        \n",
    "        try:\n",
    "            # Click cookie banner if it appears\n",
    "            driver.find_element(\"xpath\", '//*[@id=\"didomi-notice-agree-button\"]').click()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        html = driver.page_source\n",
    "        soup = bs(html, 'lxml')\n",
    "        \n",
    "        # Stop if the page does not exist or we are redirected\n",
    "        if soup.find('main', {'class':'listing-items'}) is None:\n",
    "            print(\"No more items found. Ending ID scrape.\")\n",
    "            break\n",
    "\n",
    "        articles = soup.find('main',{'class':'listing-items'}).find_all('article')\n",
    "        if not articles:\n",
    "            print(\"No articles found on this page. Ending ID scrape.\")\n",
    "            break\n",
    "\n",
    "        for article in articles:\n",
    "            data_id = article.get('data-element-id')\n",
    "            if data_id:\n",
    "                id_list.append(data_id)\n",
    "\n",
    "        # A small delay to mimic human behavior\n",
    "        time.sleep(random.randint(1, 2))\n",
    "        page_number += 1\n",
    "        \n",
    "    driver.quit()\n",
    "    \n",
    "    # Remove duplicates and None values\n",
    "    id_list = list(set([article for article in id_list if article is not None]))\n",
    "    print(f\"Found a total of {len(id_list)} unique IDs.\")\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Parse Property Details\n",
    "\n",
    "The `parse_property_details` function takes a single property ID and a Selenium driver instance. It navigates to the property's specific URL and extracts all relevant features like price, location, area, number of rooms, and amenities (terrace, elevator, etc.). It handles potential errors gracefully and returns the scraped data as a single-row Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_property_details(property_id, driver):\n",
    "    \"\"\"\n",
    "    Parses the details of a single property page.\n",
    "    \n",
    "    Args:\n",
    "        property_id (str): The unique ID of the property.\n",
    "        driver: The Selenium WebDriver instance.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the details of the property.\n",
    "    \"\"\"\n",
    "    print(f\"Parsing data for property ID: {property_id}\")\n",
    "    url = f\"https://www.idealista.com/en/inmueble/{property_id}/\"\n",
    "    driver.get(url)\n",
    "    time.sleep(random.randint(5, 10))\n",
    "    \n",
    "    try:\n",
    "        driver.find_element(\"xpath\", '//*[@id=\"didomi-notice-agree-button\"]').click()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = bs(html, 'lxml')\n",
    "    \n",
    "    details_dict = {}\n",
    "    details_dict['id'] = property_id\n",
    "    \n",
    "    # --- Basic Info ---\n",
    "    try:\n",
    "        details_dict['title'] = soup.select_one('.main-info__title-main').text if soup.select_one('.main-info__title-main') else \"NaN\"\n",
    "        location_str = soup.select_one('.main-info__title-minor')\n",
    "        if location_str:\n",
    "            details_dict['location'] = location_str.text.split(',')[0].replace(',','')\n",
    "            details_dict['city'] = location_str.text.split(',')[1] if len(location_str.text.split(',')) > 1 else \"NaN\"\n",
    "        else:\n",
    "            details_dict['location'], details_dict['city'] = \"NaN\", \"NaN\"\n",
    "            \n",
    "        price_str = soup.select_one(\".info-data-price span.h1-simulated\")\n",
    "        details_dict['price'] = price_str.text.replace('.', '') if price_str else \"NaN\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing main info: {e}\")\n",
    "\n",
    "    # --- Features ---\n",
    "    try:\n",
    "        info_features = soup.find(\"div\", {\"class\": \"info-features\"}).find_all('span')\n",
    "        for feature in info_features:\n",
    "            feature_text = feature.text.strip()\n",
    "            if \"m²\" in feature_text:\n",
    "                details_dict['area_m2'] = feature_text.split()[0]\n",
    "            elif \"hab.\" in feature_text:\n",
    "                details_dict['rooms'] = feature_text.split()[0]\n",
    "            elif \"Planta\" in feature_text or \"Bajo\" in feature_text:\n",
    "                details_dict['floor_info'] = feature_text\n",
    "    except Exception as e:\n",
    "        pass \n",
    "\n",
    "    # --- Additional Details & Amenities ---\n",
    "    try:\n",
    "        details_propertys = soup.find(\"div\", {\"class\": \"details-property\"})\n",
    "        if details_propertys:\n",
    "            full_details_text = \" \".join([p.text for p in details_propertys.find_all('p')]).lower()\n",
    "            details_dict['air_conditioning'] = 'aire acondicionado' in full_details_text\n",
    "            details_dict['terrace'] = 'terraza' in full_details_text\n",
    "            details_dict['storage'] = 'trastero' in full_details_text\n",
    "            details_dict['elevator'] = 'ascensor' in full_details_text\n",
    "            details_dict['garage'] = 'garaje' in full_details_text\n",
    "            details_dict['pool'] = 'piscina' in full_details_text\n",
    "            details_dict['garden'] = 'jardín' in full_details_text or 'zonas verdes' in full_details_text\n",
    "            \n",
    "            if \"obra nueva\" in full_details_text:\n",
    "                details_dict['status'] = \"New build\"\n",
    "            elif \"buen estado\" in full_details_text:\n",
    "                details_dict['status'] = \"Good condition\"\n",
    "            elif \"a reformar\" in full_details_text:\n",
    "                details_dict['status'] = \"To renovate\"\n",
    "            else:\n",
    "                details_dict['status'] = \"Not specified\"\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        \n",
    "    df = pd.DataFrame([details_dict])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Execution\n",
    "\n",
    "This section runs the scraping process.\n",
    "\n",
    "### 3.1. How to Find the Target Zone\n",
    "\n",
    "The `TARGET_ZONE` variable defines which geographical area to scrape. To find the correct value for a specific district in Barcelona:\n",
    "\n",
    "1.  Go to the Idealista map for Barcelona: [https://www.idealista.com/venta-viviendas/barcelona-barcelona/mapa](https://www.idealista.com/venta-viviendas/barcelona-barcelona/mapa)\n",
    "2.  Click on the desired district on the map.\n",
    "3.  Look at the URL in your browser's address bar.\n",
    "4.  Copy the part of the path that comes after `.../venta-viviendas/`. For example, for the \"Gràcia\" district, the path will be `barcelona/gracia`.\n",
    "\n",
    "Once you have the zone, set it in the configuration cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Define the zone you want to scrape.\n",
    "# You can get this path from the Idealista map URL for Barcelona, as explained above.\n",
    "\n",
    "# List of available zones for Barcelona:\n",
    "# 'barcelona/ciutat-vella'\n",
    "# 'barcelona/eixample'\n",
    "# 'barcelona/sants-montjuic'\n",
    "# 'barcelona/les-corts'\n",
    "# 'barcelona/sarria-sant-gervasi'\n",
    "# 'barcelona/gracia'\n",
    "# 'barcelona/horta-guinardo'\n",
    "# 'barcelona/nou-barris'\n",
    "# 'barcelona/sant-andreu'\n",
    "# 'barcelona/sant-marti'\n",
    "\n",
    "TARGET_ZONE = 'barcelona/sarria-sant-gervasi'\n",
    "\n",
    "# --- Step 1: Get all property IDs ---\n",
    "# This function can take a long time to run.\n",
    "# It's recommended to run it once and save the results.\n",
    "id_list = get_id_list(TARGET_ZONE)\n",
    "\n",
    "# Save the IDs to a CSV file for backup and later use\n",
    "ids_df = pd.DataFrame(id_list, columns=['id'])\n",
    "ids_filename = f'data/id_list_{TARGET_ZONE.replace(\"/\", \"_\")}.csv'\n",
    "ids_df.to_csv(ids_filename, index=False)\n",
    "\n",
    "print(f\"Saved {len(ids_df)} IDs to {ids_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Parse Details for Each Property ID\n",
    "\n",
    "Now that we have the list of IDs, we will loop through each one, call our `parse_property_details` function to scrape its specific page, and append the results to a final DataFrame. This process is intentionally slow, with pauses between requests to be respectful to Idealista's servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Parse details for each ID ---\n",
    "\n",
    "# Define the target zone to construct the correct filename\n",
    "TARGET_ZONE = 'barcelona/sarria-sant-gervasi'\n",
    "\n",
    "# Construct the filename and load the previously saved property IDs\n",
    "ids_filename = f'../data/id_list_{TARGET_ZONE.replace(\"/\", \"_\")}.csv'\n",
    "ids_df = pd.read_csv(ids_filename)\n",
    "\n",
    "print(f\"Loaded {len(ids_df)} IDs from '{ids_filename}'\")\n",
    "\n",
    "# --- Main scraping loop ---\n",
    "all_properties_df = pd.DataFrame()\n",
    "driver = uc.Chrome() # Initialize the browser once\n",
    "\n",
    "for property_id in ids_df['id']:\n",
    "    try:\n",
    "        temp_df = parse_property_details(property_id, driver)\n",
    "        all_properties_df = pd.concat([all_properties_df, temp_df], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"A critical error occurred for ID {property_id}: {e}. Skipping.\")\n",
    "    \n",
    "    # A random pause to be respectful to the server\n",
    "    time.sleep(random.randint(4, 8))\n",
    "\n",
    "driver.quit() # Close the browser when finished\n",
    "\n",
    "print(\"Scraping finished.\")\n",
    "all_properties_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Final Data\n",
    "\n",
    "Finally, we save the complete dataset into a single CSV file for the next stage of the analysis (data cleaning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save the complete dataset ---\n",
    "final_filename = f'../data/scraped_data_{TARGET_ZONE.replace(\"/\", \"_\")}.csv'\n",
    "all_properties_df.to_csv(final_filename, index=False)\n",
    "\n",
    "print(f\"Complete dataset saved to {final_filename}\")\n",
    "all_properties_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Utility: Resuming an Interrupted Scrape\n",
    "\n",
    "Web scraping can often be interrupted due to network errors. The code below provides a utility to resume the process from the last successfully scraped item, so you don't have to start from scratch.\n",
    "\n",
    "To use it, you would typically uncomment the code and run these cells manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- SCRIPT TO RESUME A FAILED SCRAPE ---\n",
    "\n",
    "# # 1. Load the dataframe of already scraped properties\n",
    "# main_df_filename = f'../data/scraped_data_{TARGET_ZONE.replace(\"/\", \"_\")}.csv'\n",
    "# main_df = pd.read_csv(main_df_filename)\n",
    "\n",
    "# # 2. Load the complete list of all property IDs\n",
    "# ids_filename = f'../data/id_list_{TARGET_ZONE.replace(\"/\", \"_\")}.csv'\n",
    "# ids_df = pd.read_csv(ids_filename)\n",
    "\n",
    "# # 3. Find which IDs are missing by comparing the two lists\n",
    "# scraped_ids = set(main_df['id'].astype(str))\n",
    "# all_ids = set(ids_df['id'].astype(str))\n",
    "# missing_ids = all_ids - scraped_ids\n",
    "\n",
    "# print(f\"Previously scraped: {len(scraped_ids)} properties.\")\n",
    "# print(f\"Found {len(missing_ids)} IDs to resume scraping.\")\n",
    "\n",
    "# # 4. Scrape only the missing IDs\n",
    "# if missing_ids:\n",
    "#     resumed_properties_df = pd.DataFrame()\n",
    "#     driver = uc.Chrome()\n",
    "\n",
    "#     for property_id in list(missing_ids):\n",
    "#         try:\n",
    "#             temp_df = parse_property_details(property_id, driver)\n",
    "#             resumed_properties_df = pd.concat([resumed_properties_df, temp_df], ignore_index=True)\n",
    "#         except Exception as e:\n",
    "#             print(f\"A critical error occurred for ID {property_id}: {e}. Skipping.\")\n",
    "#         time.sleep(random.randint(4, 8))\n",
    "\n",
    "#     driver.quit()\n",
    "#     print(f\"Finished scraping {len(resumed_properties_df)} new properties.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine and Save Recovered Data\n",
    "\n",
    "After running the recovery script, you can combine the original dataframe with the newly scraped data and save the final, complete version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Combine original and resumed data ---\n",
    "\n",
    "# # 1. Check if there is new data to combine\n",
    "# if not resumed_properties_df.empty:\n",
    "#     # 2. Concatenate the two dataframes\n",
    "#     final_df = pd.concat([main_df, resumed_properties_df], ignore_index=True)\n",
    "\n",
    "#     # 3. Clean up the dataframe by removing any unnamed columns and resetting the index\n",
    "#     final_df = final_df.loc[:, ~final_df.columns.str.contains('^Unnamed')]\n",
    "#     final_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#     # 4. Save the final, complete file\n",
    "#     final_filename = f'../data/scraped_data_{TARGET_ZONE.replace(\"/\", \"_\")}_complete.csv'\n",
    "#     final_df.to_csv(final_filename, index=False)\n",
    "\n",
    "#     print(f\"Successfully saved combined dataset with {len(final_df)} properties to {final_filename}.\")\n",
    "#     final_df.info()\n",
    "# else:\n",
    "#     print(\"No new properties were scraped. The original dataset is already complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
